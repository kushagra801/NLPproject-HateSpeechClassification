{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NegativeSampling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VZXi_KGi0UR"
      },
      "source": [
        "# Task 1: Word Embeddings (10 points)\n",
        "\n",
        "This notebook will guide you through all steps necessary to train a word2vec model (Detailed description in the PDF)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48t-II1vkuau"
      },
      "source": [
        "## Imports\n",
        "\n",
        "This code block is reserved for your imports. \n",
        "\n",
        "You are free to use the following packages: \n",
        "\n",
        "(List of packages)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kh6nh84-AOL"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWmk3hVllEcU"
      },
      "source": [
        "# 1.1 Get the data (0.5 points)\n",
        "\n",
        "The Hindi portion HASOC corpus from [github.io](https://hasocfire.github.io/hasoc/2019/dataset.html) is already available in the repo, at data/hindi_hatespeech.tsv . Load it into a data structure of your choice. Then, split off a small part of the corpus as a development set (~100 data points).\n",
        "\n",
        "If you are using Colab the first two lines will let you upload folders or files from your local file system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHcNeyKi-AOQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "68182127-434a-4e92-ef0a-062087a350e8"
      },
      "source": [
        "#FOR HINDI DATASET\n",
        "url = 'https://raw.githubusercontent.com/alch00001/NNTI-WS2021-NLP-Project/main/data/hindi_hatespeech.tsv'\n",
        "development = pd.read_csv(url, sep='\\t')\n",
        "development.astype(\"string\")\n",
        "development.head()\n",
        "\n",
        "development['text'] = development['text'].apply(lambda x:' '.join(x.lower() for x in x.split())) #remove english words\n",
        "development['text'] = development['text'].apply(lambda x: regex.sub(r'(#[^\\s]*)*', '',x))                                    #removing hashtags     \n",
        "development['text'] = development['text'].apply(lambda x: regex.sub(r'(@[\\w]*)*[\\d~\\|\\p{Punct}*]*(http[^\\s]*)*', '',x)) \n",
        "development['text'] = development['text'].apply(lambda x: regex.sub(r'<[^<]+?>','',x)) #remove html \n",
        "development['text'] = development['text'].apply(lambda x: regex.sub(r'href=','',x)) \n",
        "development['text'] = development['text'].apply(lambda x: x.lower())                                                          #make lower case\n",
        "#import and remove stopwords\n",
        "stopurl = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-hi/master/stopwords-hi.txt'\n",
        "stopwords = pd.read_csv(stopurl, sep='\\t', header=None)\n",
        "development['text'] = development['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords[:][0].tolist()))\n",
        "#remove emojis, this script was taken from github\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "development['text'] = development['text'].apply(lambda x: remove_emoji(x))\n",
        "\n",
        "#this is only unique words\n",
        "V = list(set(development['text'].str.split(' ').sum()))\n",
        "#all words\n",
        "corpus = ' '.join([i for i in development['text']]).split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ab022a4b93fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mstopurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://raw.githubusercontent.com/stopwords-iso/stopwords-hi/master/stopwords-hi.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdevelopment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevelopment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#remove emojis, this script was taken from github\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_emoji\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ab022a4b93fd>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mstopurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://raw.githubusercontent.com/stopwords-iso/stopwords-hi/master/stopwords-hi.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdevelopment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevelopment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#remove emojis, this script was taken from github\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_emoji\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ab022a4b93fd>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mstopurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://raw.githubusercontent.com/stopwords-iso/stopwords-hi/master/stopwords-hi.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdevelopment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevelopment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#remove emojis, this script was taken from github\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_emoji\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGzDMwh_OUFy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "8e033120-c041-4658-9d85-f2e33a534123"
      },
      "source": [
        "#FOR BENGALI DATASET\n",
        "'''\n",
        "url = 'https://raw.githubusercontent.com/alch00001/NNTI-WS2021-NLP-Project/main/data/bengali_hatespeech.csv'\n",
        "bengali_data = pd.read_csv(url,header = None)\n",
        "\n",
        "#Making equal to Hindi set which has 2469 hate  and 2196 non hate\n",
        "#lets just make 4700 Bengali corpus, 2500 hate and 2200 not hate\n",
        "development = bengali_data.loc[1:2500].copy()\n",
        "not_hate = bengali_data.loc[10001:12200].copy()\n",
        "development = development.append(not_hate)\n",
        "development = development.sample(frac=1) #shuffle it so its random\n",
        "\n",
        "#preprocess as before\n",
        "development[0] = development[0].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "development[0] = development[0].apply(lambda x: regex.sub(r'(#[^\\s]*)*', '',x))                                    #removing hashtags     \n",
        "development[0] = development[0].apply(lambda x: regex.sub(r'(@[\\w]*)*[\\d~\\|\\p{Punct}*]*(http[^\\s]*)*', '',x)) \n",
        "development[0] = development[0].apply(lambda x: regex.sub(r'<[^<]+?>','',x)) #remove html \n",
        "development[0] = development[0].apply(lambda x: regex.sub(r'href=','',x))    \n",
        "development[0] = development[0].apply(lambda x: regex.sub(r'\\s{2,}',' ',x)) \n",
        "development[0] = development[0].str.replace('\\s{2,}', ' ')                                                    #make lower case\n",
        "#import and remove stopwords\n",
        "stopurl = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-bn/master/stopwords-bn.txt'\n",
        "stopwords = pd.read_csv(stopurl, sep='\\t', header=None)\n",
        "development[0] = development[0].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords[:][0].tolist()))\n",
        "#remove emojis, this script was taken from github\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "development[0] = development[0].apply(lambda x: remove_emoji(x))\n",
        " \n",
        "development[0] = development[0].replace(r'^\\s*$', '', regex=True)\n",
        "print(development[0].head())\n",
        "\n",
        "V = list(set(development[0].str.split(' ').sum()))\n",
        "corpus = ' '.join([i for i in development[0]]).split()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nurl = \\'https://raw.githubusercontent.com/alch00001/NNTI-WS2021-NLP-Project/main/data/bengali_hatespeech.csv\\'\\nbengali_data = pd.read_csv(url,header = None)\\n\\n#Making equal to Hindi set which has 2469 hate  and 2196 non hate\\n#lets just make 4700 Bengali corpus, 2500 hate and 2200 not hate\\ndevelopment = bengali_data.loc[1:2500].copy()\\nnot_hate = bengali_data.loc[10001:12200].copy()\\ndevelopment = development.append(not_hate)\\ndevelopment = development.sample(frac=1) #shuffle it so its random\\n\\n#preprocess as before\\ndevelopment[0] = development[0].apply(lambda x: \" \".join(x.lower() for x in x.split()))\\ndevelopment[0] = development[0].apply(lambda x: regex.sub(r\\'(#[^\\\\s]*)*\\', \\'\\',x))                                    #removing hashtags     \\ndevelopment[0] = development[0].apply(lambda x: regex.sub(r\\'(@[\\\\w]*)*[\\\\d~\\\\|\\\\p{Punct}*]*(http[^\\\\s]*)*\\', \\'\\',x)) \\ndevelopment[0] = development[0].apply(lambda x: regex.sub(r\\'<[^<]+?>\\',\\'\\',x)) #remove html \\ndevelopment[0] = development[0].apply(lambda x: regex.sub(r\\'href=\\',\\'\\',x))    \\ndevelopment[0] = development[0].apply(lambda x: regex.sub(r\\'\\\\s{2,}\\',\\' \\',x)) \\ndevelopment[0] = development[0].str.replace(\\'\\\\s{2,}\\', \\' \\')                                                    #make lower case\\n#import and remove stopwords\\nstopurl = \\'https://raw.githubusercontent.com/stopwords-iso/stopwords-bn/master/stopwords-bn.txt\\'\\nstopwords = pd.read_csv(stopurl, sep=\\'\\t\\', header=None)\\ndevelopment[0] = development[0].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords[:][0].tolist()))\\n#remove emojis, this script was taken from github\\ndef remove_emoji(text):\\n    emoji_pattern = re.compile(\"[\"\\n                           u\"üòÄ-üôè\"  # emoticons\\n                           u\"üåÄ-üóø\"  # symbols & pictographs\\n                           u\"üöÄ-\\U0001f6ff\"  # transport & map symbols\\n                           u\"\\U0001f1e0-üáø\"  # flags \\n                           u\"‚úÇ-‚û∞\"\\n                           u\"‚ìÇ-üâë\"\\n                           \"]+\", flags=re.UNICODE)\\n    return emoji_pattern.sub(r\\'\\', text)\\ndevelopment[0] = development[0].apply(lambda x: remove_emoji(x))\\n \\ndevelopment[0] = development[0].replace(r\\'^\\\\s*$\\', \\'\\', regex=True)\\nprint(development[0].head())\\n\\nV = list(set(development[0].str.split(\\' \\').sum()))\\ncorpus = \\' \\'.join([i for i in development[0]]).split()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3LlxacbMOQ6",
        "outputId": "91df422b-cefd-4830-fd5a-008a16c7d851"
      },
      "source": [
        "if len(V[0]) == 0: #resolve an issue with emptry string appearing in vocab\n",
        "  V.pop(0)\n",
        "\n",
        "word2idx = {w: idx for (idx, w) in enumerate(V)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(V)}\n",
        "print(\"V is of length \",len(V))\n",
        "print(len(corpus))\n",
        "print(V[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "V is of length  6485\n",
            "15991\n",
            "‡§∂‡§ø‡§µ‡§∏‡•á‡§®‡§æ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiaVglVNoENY"
      },
      "source": [
        "* Then, write a function ```word_to_one_hot``` that returns a one-hot encoding of an arbitrary word in the vocabulary. The size of the one-hot encoding should be ```len(v)```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqPNw6IT-AOQ",
        "outputId": "5ae474cc-db3f-45a1-bc87-8c7674983453"
      },
      "source": [
        "#TODO: implement!\n",
        "\n",
        "def word_to_one_hot(word):\n",
        "  one_hot = [0 if word != x else 1 for x in V]\n",
        "  return one_hot\n",
        "\n",
        "X = word_to_one_hot(V[1])\n",
        "len(X)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6485"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKD8zBlxVclh"
      },
      "source": [
        "## 1.4 Subsampling (0.5 points)\n",
        "\n",
        "The probability to keep a word in a context is given by:\n",
        "\n",
        "$P_{keep}(w_i) = \\Big(\\sqrt{\\frac{z(w_i)}{0.001}}+1\\Big) \\cdot \\frac{0.001}{z(w_i)}$\n",
        "\n",
        "Where $z(w_i)$ is the relative frequency of the word $w_i$ in the corpus. Now,\n",
        "* Calculate word frequencies\n",
        "* Define a function ```sampling_prob``` that takes a word (string) as input and returns the probabiliy to **keep** the word in a context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj4sDOVMMr0b"
      },
      "source": [
        "#TODO: implement!\n",
        "def word_frequency(word):\n",
        "  freq = 0\n",
        "  for x in corpus:\n",
        "    if x == word:\n",
        "      freq += 1\n",
        "  return freq\n",
        "\n",
        "def sampling_prob(word):\n",
        "  relative_frq = word_frequency(word)/len(V)\n",
        "  if relative_frq==0:          #if word is not present in the corpus\n",
        "    return 0\n",
        "  else:\n",
        "    p_keep = (np.sqrt(relative_frq/0.000001)+1)*(0.000001/relative_frq)\n",
        "    return p_keep\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKJm5A-9T7FS"
      },
      "source": [
        "#Challenge Task 3 - implementing negative sampling\n",
        "\n",
        "def gen_negative_sample_table():\n",
        "  exponent = 0.75\n",
        "  normlization_factor = sum([math.pow(word_frequency(word), exponent) for word in V])\n",
        "  table_size = int(1e5) #make it to 1e8\n",
        "  table = np.zeros(table_size, dtype=np.int)\n",
        "  p = 0 # Cumulative probability\n",
        "  i = 0\n",
        "  for j, unigram in enumerate(word2idx):\n",
        "      p += float(math.pow(word_frequency(unigram), exponent))/normlization_factor\n",
        "      while i < table_size and float(i) / table_size < p:\n",
        "        table[i] = j\n",
        "        i += 1 \n",
        "  return table\n",
        "\n",
        "neg_samples = gen_negative_sample_table()\n",
        "np.random.shuffle(neg_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN_LkNGfmNlr"
      },
      "source": [
        "#generate 5 words for negative samples\n",
        "def get_neg_sample(table,count):\n",
        "    indices = np.random.randint(low=0, high=len(table), size=count)\n",
        "    return [V[table[i]] for i in indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxV1P90zplxu"
      },
      "source": [
        "# 1.5 Skip-Grams (1 point)\n",
        "\n",
        "Now that you have the vocabulary and one-hot encodings at hand, you can start to do the actual work. The skip gram model requires training data of the shape ```(current_word, context)```, with ```context``` being the words before and/or after ```current_word``` within ```window_size```. \n",
        "\n",
        "* Have closer look on the original paper. If you feel to understand how skip-gram works, implement a function ```get_target_context``` that takes a sentence as input and [yield](https://docs.python.org/3.9/reference/simple_stmts.html#the-yield-statement)s a ```(current_word, context)```.\n",
        "\n",
        "* Use your ```sampling_prob``` function to drop words from contexts as you sample them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8CCTpVy-AOR"
      },
      "source": [
        "#TODO: implement!\n",
        "window_size = 5\n",
        "\n",
        "def get_target_context(sentence):\n",
        "  words = sentence.split()\n",
        "  training_data = []\n",
        "  #remove high freq words\n",
        "  for word in words:\n",
        "    if random.random() < sampling_prob(word): \n",
        "      words.remove(word)\n",
        "  #for actual(positive) contexts\n",
        "  for word in words:\n",
        "    position = words.index(word) \n",
        "    for i in range(-window_size, window_size+1):\n",
        "      if position+i<0 or position+i>=len(words) or i==0:\n",
        "        continue\n",
        "      training_data.append([word, words[position+i],1])  \n",
        "  #now get random(negative) contexts\n",
        "  neg_data = []\n",
        "  for word in words:\n",
        "    n_neg_examples = 0\n",
        "    negs = get_neg_sample(neg_samples,10)\n",
        "    for n in negs:\n",
        "      if n_neg_examples >= 5:\n",
        "        break\n",
        "      elif (word,n) not in training_data:\n",
        "        neg_data.append([word,n,0])\n",
        "        n_neg_examples +=1\n",
        "  yield training_data, neg_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfEFgtkmuDjL"
      },
      "source": [
        "# 1.6 Hyperparameters (0.5 points)\n",
        "\n",
        "According to the word2vec paper, what would be a good choice for the following hyperparameters? \n",
        "\n",
        "* Embedding dimension\n",
        "* Window size\n",
        "\n",
        "Initialize them in a dictionary or as independent variables in the code block below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7xSKuFJcYoD"
      },
      "source": [
        "# Set hyperparameters\n",
        "window_size = 5\n",
        "embedding_size = 300 \n",
        "vocab_size = len(V)\n",
        "batch_size = 7000\n",
        "learning_rate = 0.01\n",
        "epochs = 8\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiM2zq-YunPx"
      },
      "source": [
        "# 1.7 Pytorch Module (0.5 + 0.5 + 0.5 points)\n",
        "\n",
        "Pytorch provides a wrapper for your fancy and super-complex models: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The code block below contains a skeleton for such a wrapper. Now,\n",
        "\n",
        "* Initialize the two weight matrices of word2vec as fields of the class.\n",
        "\n",
        "* Override the ```forward``` method of this class. It should take a one-hot encoding as input, perform the matrix multiplications, and finally apply a log softmax on the output layer.\n",
        "\n",
        "* Initialize the model and save its weights in a variable. The Pytorch documentation will tell you how to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsSqOoGGVDxu"
      },
      "source": [
        "#Create Model\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    \n",
        "    # Hidden layer\n",
        "    self.l1 = nn.Linear(len(V), embedding_size, bias=False)\n",
        "    self.l2 = nn.Linear(len(V), embedding_size, bias=False)\n",
        "\n",
        "  def forward(self, targ, context):\n",
        "    Z1 = self.l1(targ)\n",
        "    Z2 = self.l2(context)\n",
        "    dot_u_v = torch.zeros(targ.shape[0], 1)\n",
        "    for j in range(len(Z1)):\n",
        "      dot_u_v[j, :] = torch.dot(Z1[j, :],Z2[j, :])\n",
        "    pred = dot_u_v\n",
        "    out = nn.Sigmoid()(pred)\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XefIDMMHv5zJ"
      },
      "source": [
        "# 1.8 Loss function and optimizer (0.5 points)\n",
        "\n",
        "Initialize variables with [optimizer](https://pytorch.org/docs/stable/optim.html#module-torch.optim) and loss function. You can take what is used in the word2vec paper, but you can use alternative optimizers/loss functions if you explain your choice in the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9-Ino-e29w3"
      },
      "source": [
        "# Define optimizer and loss\n",
        "model = Word2Vec()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,)\n",
        "criterion = nn.BCELoss()\n",
        "model = model.to(device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckTfK78Ew8wI"
      },
      "source": [
        "# 1.9 Training the model (3 points)\n",
        "\n",
        "As everything is prepared, implement a training loop that performs several passes of the data set through the model. You are free to do this as you please, but your code should:\n",
        "\n",
        "* Load the weights saved in 1.6 at the start of every execution of the code block\n",
        "* Print the accumulated loss at least after every epoch (the accumulate loss should be reset after every epoch)\n",
        "* Define a criterion for the training procedure to terminate if a certain loss value is reached. You can find the threshold by observing the loss for the development set.\n",
        "\n",
        "You can play around with the number of epochs and the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cDXH3tr8bom",
        "outputId": "5a3492b1-e1a9-4b77-bee8-4a3f8c82d28d"
      },
      "source": [
        "#iterate through every sentence and create a list \n",
        "#of target,context pairs stored in variable 'pairs'\n",
        "train_set = development['text'].str.split(',')\n",
        "train_set = train_set.to_list()\n",
        "print(\"sentences :\",len(train_set))\n",
        "train_set = [''.join(x) for x in train_set]\n",
        "print('train set created')\n",
        "pos_pairs = []\n",
        "neg_pairs = []\n",
        "for sentence in train_set:\n",
        "  contexts = get_target_context(sentence)\n",
        "  pos, neg = next(contexts)\n",
        "  pos_pairs += pos\n",
        "  neg_pairs += neg\n",
        "\n",
        "print('pairs formed')\n",
        "print('len positive: ', len(pos_pairs))\n",
        "print('len negative: ', len(neg_pairs))\n",
        "pairs = pos_pairs + neg_pairs\n",
        "random.shuffle(pairs)\n",
        "print(pairs[:10])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentences : 1001\n",
            "train set created\n",
            "pairs formed\n",
            "len positive:  122091\n",
            "len negative:  76215\n",
            "[['‡§á‡§§‡§®‡§æ', 'apman', 0], ['‡§¶‡§ø‡§≤‡§æ‡§è‡§Å‡§ó‡•á', '‡§™‡§æ‡§è‡§Å‡§ó‡•á', 1], ['‡§ó‡§†‡§¨‡§Ç‡§ß‡§®', '‡§ß‡•á‡§∞‡•ç‡§Ø', 1], ['‡§ñ‡§ø‡§∞‡§æ‡§ú', '‡§∂‡§π‡•Ä‡§¶', 1], ['‡§Ü‡§≤‡•ã‡§ï‡§ß‡§®‡•ç‡§µ‡§æ', '‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§®', 1], ['‡§∞‡•Å‡§™‡§Ø‡•á', '‡§®‡•å‡§ï‡§∞‡•Ä', 1], ['‡§Ö‡§µ‡§æ‡§∞‡•ç‡§°', '‡§™‡§§‡§æ', 1], ['‡§∂‡•ç‡§∞‡•Ä‡§∞‡§æ‡§Æ', '‡§µ‡§®‡•ç‡§¶‡•á‡§Æ‡§æ‡§§‡§∞‡§Æ', 1], ['‡§∂‡§æ‡§ñ‡§º', '‡§¨‡§∞‡§ó‡§¶', 1], ['‡§¶‡•á‡§∂‡§µ‡§ø‡§∞‡•ã‡§ß‡•Ä', '‡§®‡§à‡§Æ', 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "r-xsbxIPsM3O",
        "outputId": "aca70b33-41ff-4579-bc35-26e4bf24b002"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "dataloader = DataLoader(\n",
        "    pairs, \n",
        "    batch_size=100)\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  for i,(center, contexts,labels)in enumerate(dataloader): #returns a list of word,context pairs in string format of length batch\n",
        "      #convert target/center words to one hot encodings\n",
        "      word = torch.FloatTensor([word_to_one_hot(x) for x in center])\n",
        "      context = torch.FloatTensor([word_to_one_hot(x) for x in contexts])\n",
        "      output = model(word.to(device=device),context.to(device=device))\n",
        "      labels = labels.float()\n",
        "      optimizer.zero_grad()\n",
        "      loss = criterion(output,torch.Tensor(labels).view(output.shape[0], 1))\n",
        "      total_loss += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "  print(\"Total loss at epoch: \",epoch+1, \"loss: \", total_loss/(len(dataloader)))\n",
        "\n",
        "print(\"Training finished\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-dec34e7deb57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;31m#convert target/center words to one hot encodings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7q_p9QuofVH"
      },
      "source": [
        "w1 = model.l1.weight\n",
        "print(w1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgQkaYstyj0Q"
      },
      "source": [
        "# 1.10 Train on the full dataset (0.5 points)\n",
        "\n",
        "Now, go back to 1.1 and remove the restriction on the number of sentences in your corpus. Then, reexecute code blocks 1.2, 1.3 and 1.6 (or those relevant if you created additional ones). \n",
        "\n",
        "* Then, retrain your model on the complete dataset.\n",
        "\n",
        "* Now, the input weights of the model contain the desired word embeddings! Save them together with the corresponding vocabulary items (Pytorch provides a nice [functionality](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for this)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x8hQP_bg4_g"
      },
      "source": [
        "#Saving Model\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "model_save_name = 'classifier.pth'\n",
        "#path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "#torch.save(model.state_dict(), path)\n",
        "\n",
        "torch.save(model.state_dict(), 'checkpoint.pth') #saving weights\n",
        "state_dict = torch.load('checkpoint.pth')\n",
        "print(state_dict.keys())\n",
        "print(state_dict['l2.weight'].shape)\n",
        "weights = state_dict['l1.weight']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YtKSWqlCIXv"
      },
      "source": [
        "#create dictionary mapping vocab words to tensors\n",
        "weights = torch.transpose(weights,0,1)\n",
        "vectors = {}\n",
        "w = weights.cpu()\n",
        "for i in range(len(V)):\n",
        "  vectors[V[i]] = w[i].numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wMdpgtjavIw"
      },
      "source": [
        "#Write these dictionary to text file in the format:\n",
        "# word tensorval1 tensorval2 ....tensorval2\n",
        "from itertools import chain\n",
        "import io\n",
        "with open(r'C:\\Users\\kusha\\Desktop\\HindiEmbeddingsUpdated.txt', 'w', encoding=\"utf-8\") as f:\n",
        "  f.write(\"\\n\".join(\" \".join(chain([key],[str(number) for number in value])) for key,value in vectors.items()))  \n",
        "f.close()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D54j8H30HBN"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whut5f3B0qCg"
      },
      "source": [
        "torch.cuda.get_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}