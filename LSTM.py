# -*- coding: utf-8 -*-
"""LSTMforHindi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tOyOWzCX5FxG4V-ewy6k4nKIpf47v5QD
"""

import string
import re
import numpy as np
import math
import random
import regex
from torchtext import data
import pandas as pd
import torch
import torch.autograd as autograd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn
import torchtext.vocab as vocab
from DataPreprocess import *
#from TrainEvalLoops import *
#from model import *

#Here we are implementing the same data cleaning process as in the word embeddings
#so there will be no mismatch between our imported word embeddings 
#and we need to preprocess the sentences to be sent through the network
url = 'Data/bengali_hatespeech.csv'
df = pd.read_csv(url,header = None)
df = reduce_bengali(df)
#clean data
df[0] = clean_data(df[0])
df[0] = remove_bengali_stopwords(df[0])
df[0].replace('', np.nan, inplace=True)
df.dropna(subset=[0], inplace=True)
df = bengali_drop_columns(df) #dropping columns we don't need 
#load our custom embeddings into vocab vector

custom_embeddings = vocab.Vectors(name = 'Data/BengaliEmb.txt') #basic skipgram embeddings

TEXT = data.Field(include_lengths = True)
LABEL = data.LabelField(dtype=torch.float)

df.columns = ['text','label'] 
fields = { 'text' : TEXT,  'label':LABEL  }
train_ds = DataFrameDataset(df, fields) #convert pandas df to torchtext df

TEXT.build_vocab(train_ds, vectors = custom_embeddings)
LABEL.build_vocab(train_ds)



#stored our pretrained word embeddings in this variable to pass to our network
pretrained = TEXT.vocab.vectors

#stored our pretrained word embeddings in this variable to pass to our network
pretrained = TEXT.vocab.vectors

#make test and train splits
SEED = 32
train_size = int(0.75*len(train_ds))

val_size = len(train_ds)-train_size

#train_dataset,val_dataset = random_split(train_ds,[train_size,val_size])
train_dataset,val_dataset = train_ds.split([train_size,val_size], random_state = random.seed(SEED))

#our iterator
BATCH_SIZE = 500

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_iterator, valid_iterator = data.BucketIterator.splits(
    (train_dataset, val_dataset), 
    batch_size = BATCH_SIZE,
    sort_key = lambda x: len(x.text),
    device = device)

import torch.nn as nn


class RNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, pad_idx, dropout = 0.5 ):
        
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)
        
        self.lstm = nn.LSTM(embedding_dim, 
                           hidden_dim, 
                           num_layers=n_layers,  
                           dropout=dropout)
        
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, text, lengths):
      
        embedded = self.dropout(self.embedding(text))
        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.to('cpu'), enforce_sorted=False)
        
        packed_output, (hidden, cell) = self.lstm(packed_embedded)
        
        #unpack sequence
        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)
        
        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))
                
        return self.fc(hidden)

INPUT_DIM = len(TEXT.vocab)
EMBEDDING_DIM = 300
HIDDEN_DIM = 250
OUTPUT_DIM = 1
N_Layers = 2
PAD = TEXT.vocab.stoi[TEXT.pad_token]
UNK = TEXT.vocab.stoi[TEXT.unk_token]

model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_Layers, PAD)

#ensure embedding in hidden layer for unknown and padding are set to zero

model.embedding.weight.data.copy_(pretrained)
model.embedding.weight.data[UNK] = torch.zeros(EMBEDDING_DIM)
model.embedding.weight.data[PAD] = torch.zeros(EMBEDDING_DIM)


optimizer = optim.Adam(model.parameters(), lr=5e-3)
criterion = nn.BCEWithLogitsLoss()
model = model.to(device)
criterion = criterion.to(device)

def binary_accuracy(preds, y):
    #returns batch accuracy
    rounded_preds = torch.round(torch.sigmoid(preds))
    correct = (rounded_preds == y).float() 
    acc = correct.sum() / len(correct)
    return acc

def train(model, iterator, optimizer, criterion):
    
    epoch_loss = 0
    epoch_acc = 0
    
    model.train()
    
    for batch in iterator:
        optimizer.zero_grad()

        text,lengths = batch.text

        predictions = model(text, lengths).squeeze(1)
        
        loss = criterion(predictions.squeeze(), batch.label)
        
        acc = binary_accuracy(predictions, batch.label)
        
        loss.backward()
        
        optimizer.step()
        
        epoch_loss += loss.item()
        epoch_acc += acc.item()
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

def evaluate(model, iterator, criterion):
    
    epoch_loss = 0
    epoch_acc = 0
    
    model.eval()
    
    with torch.no_grad():
    
        for batch in iterator:

            text,lengths = batch.text
        
            predictions = model(text, lengths).squeeze(1)
            
            loss = criterion(predictions.squeeze(), batch.label)
            
            acc = binary_accuracy(predictions, batch.label)

            epoch_loss += loss.item()
            epoch_acc += acc.item()
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

N_EPOCHS = 3
#best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):

    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)
    
    print(f'Epoch: {epoch+1:02}')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')

test_loss, test_acc = evaluate(model, valid_iterator, criterion)

print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')

